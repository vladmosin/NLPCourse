{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from gensim import models, corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_line = '-------------------'\n",
    "\n",
    "def get_abstract(text):\n",
    "    parts = text.split('Abstract:')\n",
    "    assert len(parts) == 2\n",
    "    \n",
    "    return parts[1].strip()\n",
    "\n",
    "def load_texts(path='HW1.txt'):\n",
    "    texts = []\n",
    "    with open(path, 'r') as f:\n",
    "        text = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line == split_line:\n",
    "                texts.append(get_abstract(\"\\n\".join(text)))\n",
    "                text = []\n",
    "            elif len(line) > 0:\n",
    "                text.append(line)\n",
    "                \n",
    "        if len(text) > 0:\n",
    "            texts.append(get_abstract(\"\\n\".join(text)))\n",
    "            \n",
    "    return texts\n",
    "\n",
    "\n",
    "def not_number(word):\n",
    "    return len(re.split(r'.*[0-9].*', word)) == 1\n",
    "\n",
    "\n",
    "def remove_empty(words):\n",
    "    without_empty = []\n",
    "    for word in words:\n",
    "        if len(word) > 4 and not_number(word):\n",
    "            without_empty.append(word)\n",
    "            \n",
    "    return without_empty\n",
    "\n",
    "\n",
    "def word_split(texts):\n",
    "    regex = f'[{punctuation}\\n][ \\t]*|[ \\t]+'\n",
    "    texts_word = []\n",
    "    for text in texts:\n",
    "        texts_word.append(remove_empty(re.split(regex, text.lower().strip())))\n",
    "        \n",
    "    return texts_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(texts):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lem_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        lem_texts.append([lemmatizer.lemmatize(word) for word in text])\n",
    "        \n",
    "    return lem_texts\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    wout_stop_texts = []\n",
    "    for text in texts:\n",
    "        wout_stop_texts.append([word for word in text if word not in stopwords])\n",
    "        \n",
    "    return wout_stop_texts\n",
    "\n",
    "def clean_texts(texts):\n",
    "    splitted = word_split(texts)\n",
    "    lemmatized = lemmatize(splitted)\n",
    "    \n",
    "    return remove_stopwords(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_texts()\n",
    "data = clean_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for words in data:\n",
    "    all_words += words\n",
    "    \n",
    "unique_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]\n",
    "tfidf_model = models.TfidfModel(corpus)  \n",
    "tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with best results is LDA\n",
    "First let's try with 4 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.003*\"protein\" + 0.003*\"channel\"'),\n",
       " (1, '0.002*\"collagen\" + 0.002*\"duplicate\"'),\n",
       " (2, '0.006*\"spindle\" + 0.006*\"centrosome\"'),\n",
       " (3, '0.004*\"expression\" + 0.004*\"protein\"')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(tfidf, id2word=dictionary, num_topics=4)\n",
    "topics = lda.print_topics(num_words=2)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try with 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.002*\"stress\" + 0.002*\"longevity\"'),\n",
       " (1, '0.003*\"spindle\" + 0.003*\"centrosome\"'),\n",
       " (2, '0.004*\"protein\" + 0.003*\"pathway\"')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(tfidf, id2word=dictionary, num_topics=3)\n",
    "topics = lda.print_topics(num_words=2)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try with 2 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.003*\"protein\" + 0.003*\"gene\"'),\n",
       " (1, '0.002*\"protein\" + 0.002*\"signaling\"')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(tfidf, id2word=dictionary, num_topics=2)\n",
    "topics = lda.print_topics(num_words=2)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the best number of themes is 3\n",
    "1. Something about life and health\n",
    "2. Some common biology (chromosomes, etc)\n",
    "3. Sport and food"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
